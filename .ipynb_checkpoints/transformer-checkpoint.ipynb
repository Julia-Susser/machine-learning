{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "94292d3b-ca63-4b4c-823b-9dca4d4a4e4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9059a39-fabd-4219-937c-d25433a3df1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embeddings(nn.Module):\n",
    "    def __init__(self, vocab_size,d_model):\n",
    "        super().__init__\n",
    "        self.d_model = d_model\n",
    "        self.vocab_size = vocab_size \n",
    "        self.embedding = nn.Embedding(vocab_size, d_model) #initializes embeddings to be learned to training process\n",
    "    def forward(self,x):\n",
    "        return self.embedding(x) * math.sqrt(d_model) #normalize the variance of the embeddings\n",
    "\n",
    "\n",
    "#if \n",
    "\n",
    "#if feature=2i, sin(pos/1000^(2i/d_model), if feature=2i+k, cos(pos/1000^(2i/d_model)\n",
    "def Positional_Encoding(nn.Module):\n",
    "    def __init(self,seq_length,d_model,Pdrop):\n",
    "         super(LayerNormalization,self).__init__()\n",
    "        self.dropout = nn.Droupout(Pdrop)\n",
    "        #if you add self before, then it gets learned as a parameter for the model\n",
    "        pos = torch.arange(0,seq_length).unsqueeze(1) #pos --> seq_length_pos x 1, adds an extra dimension\n",
    "        div_term = torch.exp(torch.arange(0,d_model,2)*np.log(1000)/d_model) #div term --> 1 x div_term_i\n",
    "        #so there is a different pe value for each token (pos) and feature (2i,2i+1)\n",
    "        pe = np.zeros(seq_length, d_model)\n",
    "        pe[:,0:2] = torch.sin(pe/div_term)\n",
    "        pe[:,1:2] = torch.cos(pe/div_term)\n",
    "        pe = pe.unsqueeze(0) # --> (1, seq, d_model)\n",
    "        self.register_buffer('positional_encoder',pe) #self.positional_encoder is not a learnable parameter\n",
    "\n",
    "    def forward(self,x):\n",
    "        x = x + self.positional_encoder(x)\n",
    "        return self.dropout(x)\n",
    "\n",
    "\n",
    "\n",
    "class LayerNormalization(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LayerNormalization,self).__init__()\n",
    "        self.epsilon = 1**-10 #so not divide by 0\n",
    "        self.a = nn.Parameter(torch.ones(1)) #1 dimension\n",
    "        self.bias = nn.Parameter(torch.zeros(1)) #1 dimension\n",
    "\n",
    "    def forward(x):\n",
    "        mean = x.mean(dim=1, KeepDim=True) #keep all dimensions\n",
    "        std = x.std(dim=1, KeepDim=True)\n",
    "\n",
    "        return self.a*(x-mean/(std+self.epsilon)+self.bias\n",
    "\n",
    "\n",
    "#add non linearity\n",
    "class FeedForwardNetwork(nn.Module):\n",
    "    def __init__(self,d_model,Pdrop, d_ff=None)\n",
    "        if d_ff == None:\n",
    "            d_ff = d_model*4\n",
    "        super(FeedForwardNetwork, self).__init__()\n",
    "        self.linear1 = nn.Linear(d_model,d_ff)\n",
    "        self.dropout= nn.Dropout(Pdrop)\n",
    "        self.linear2 = nn.Linear(d_ff, dmodel)\n",
    "    \n",
    "    def forward(self,x):\n",
    "        x = self.dropout(F.relu(self.linear1(x)))\n",
    "        return self.linear2(x)\n",
    "\n",
    "\n",
    "class AddAndNorm(nn.Module):\n",
    "    def __init__(self,LayerNorm, Pdrop):\n",
    "        super(AddAndNorm, self).__init__()\n",
    "        self.dropout = nn.Dropout(Pdrop)\n",
    "        self.LayerNorm = LayerNorm()\n",
    "    def forward(self,x,sublayer):\n",
    "        return self.LayerNorm(x+self.dropout(sublayer))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1ae560a-1747-40fd-bac5-7d2b8e7ba064",
   "metadata": {},
   "outputs": [],
   "source": [
    "#same computation overhead as self attention but more semantic representations\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, h, Pdrop):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        if d_model % h == 0, \"d_k=d_v=d_model/h\":\n",
    "            raise ValueError()\n",
    "\n",
    "        \n",
    "        self.h = h #num_heads\n",
    "        self.d_model = d_model #length of embeddings\n",
    "        self.d_k = self.d_v = int(self.d_model / self.h)\n",
    "\n",
    "        self.dropout= nn.Dropout(Pdrop)\n",
    "\n",
    "        self.W_Q = nn.Linear(d_model,d_model)\n",
    "        self.W_K = nn.Linear(d_model,d_model)\n",
    "        self.W_V = nn.Linear(d_model,d_model)\n",
    "\n",
    "        self.W_O = nn.Linear(d_model,d_model)\n",
    "        \n",
    "    \n",
    "        \n",
    "    def forward(self,Q,K,V, mask):\n",
    "        batch_size, seq_length, d_model = Q.size()\n",
    "        \n",
    "        queries = self.W_Q(Q)\n",
    "        keys = self.W_K(K)\n",
    "        values = self.W_V(V)\n",
    "\n",
    "        queries = queries.reshape(batch_size, self.h, seq_length, self.d_k) #(batch size, seq_length, d_model) ==> (batch size, seq_length, h, d_k)\n",
    "        keys = keys.reshape(batch_size, self.h, seq_length, self.d_k)\n",
    "        values = values.reshape(batch_size, self.h, seq_length, self.d_v)\n",
    "\n",
    "        queries = queries.transpose(1,2) #(batch size, h, seq_length, d_k), flips the dims 1 and 2\n",
    "        keys = keys.transpose(1,2)\n",
    "        values = values.transpose(1,2)\n",
    "\n",
    "        scores = torch.matmul(queries, keys.transpose(-2,-1))/self.d_k**.5\n",
    "\n",
    "        scores = scores.masked_fill(mask=0, -1*10^10) #bc e^-inf=0 in softmax will be equal to 0\n",
    "        attention = F.softmax(scores, dim=-1)\n",
    "        \n",
    "        attention = self.dropout(attention) #dropout after activation function\n",
    "            \n",
    "        weighted = torch.matmul(attention, values)\n",
    "        concat = weighted.reshape(batch_size, seq_length, d_model)\n",
    "        out = self.W_O(concat) #W_O\n",
    "        return out\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b3661375-e8af-4d23-9a7a-68fa51817e47",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderBlock(nn.Module):\n",
    "    def __init__(self,d_model,h,Pdrop):\n",
    "        super(EncoderBlock, self).__init__()\n",
    "        self.MHA1 = MultiHeadAttention(d_model,h,Pdrop)\n",
    "        self.FFN = FeedForwardNetwork(d_model, Pdrop, d_ff=d_model*4)\n",
    "        self.residual = nn.ModuleList([AddAndNorm(LayerNorm, Pdrop) for x in range(2)])\n",
    "                                      \n",
    "    def forward(self,x,mask):\n",
    "       x = self.residual[0](self.MHA1(x,x,x,mask))\n",
    "       x = self.residual[1](self.FFN(x))\n",
    "       return x\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init___(self,N,d_model,h,Pdrop):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.layers = [EncoderBlock(d_model,h,Pdrop) for x in range(N)]\n",
    "    def forward(x,mask):\n",
    "        for layer in layers:\n",
    "            x = layer(x,mask)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "09cef2f7-c777-4266-8b83-e390eeb7391b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderBlock(nn.Module):\n",
    "    def __init__(self,d_model,h,Pdrop):\n",
    "        super(DecoderBlock, self).__init__()\n",
    "        self.MHA2 = MultiHeadAttention(d_model,h,Pdrop)\n",
    "        self.MHA3 = MultiHeadAttention(d_model,h,Pdrop)\n",
    "        self.FFN = FeedForwardNetwork(d_model, Pdrop, d_ff=d_model*4)\n",
    "        self.residual = nn.ModuleList([AddAndNorm(LayerNorm, Pdrop) for x in range(3)])\n",
    "    def forward(self,x,encoder_output,tgt_mask, src_mask):\n",
    "        x = self.residual[0](self.MHA2(x,x,x,tgt_mask))\n",
    "        x = self.residual[1](self.MHA3(x,encoder_output,encoder_output,src_mask))\n",
    "        x = self.residual[1](self.FFN(x))\n",
    "        return x\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init___(self,N,d_model,h,Pdrop):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.layers = [DecoderBlock(d_model,h,Pdrop) for x in range(N)]\n",
    "    def forward(x,mask):\n",
    "        for layer in layers:\n",
    "            x = layer(x,mask)\n",
    "        return x\n",
    "        \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8c94713c-3c6e-4208-b270-a2aa631a55f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProjectionLayer(nn.Module):\n",
    "    def __init__(self,vocab_size,d_model):\n",
    "        super(ProjectionLayer, self).__init__()\n",
    "        self.proj = nn.Linear(d_model,vocab_size)\n",
    "    def forward(x):\n",
    "        torch.log_softmax(self.proj(x),dim=-1) #softmax along the vocab size dim --> batch_size x seq_length, vocab_size (vocab_probabilities)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eab0a84-871b-40bf-8995-dc81ef6a1b38",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self,N,src_vocab_length,tgt_vocab_length, src_seq_length, tgt_seq_length, d_model,h,Pdrop):\n",
    "        self.encoder = Encoder(N,d_model,h,Pdrop)\n",
    "        self.decoder = Decoder(N,d_model,h,Pdrop)\n",
    "        self.src_embeddings = Embeddings(src_vocab_size, d_model)\n",
    "        self.tgt_embeddings = Embeddings(tgt_vocab_size, d_model)\n",
    "        self.src_positional_encoding = Positional_Encoding(src_seq_length,d_model,Pdrop)\n",
    "        self.tgt_positional_encoding = Positional_Encoding(tgt_seq_length,d_model,Pdrop)\n",
    "        self.projection_layer = ProjectionLayer(tgt_vocab_size,d_model)\n",
    "\n",
    "    def encode(self,tokens):\n",
    "        x = self.src_embeddings(tokens)\n",
    "        x = self.src_positional_encoding(x)\n",
    "        x = self.encoder(x)\n",
    "        return x\n",
    "        \n",
    "    def decode(self,tokens):\n",
    "        x = self.tgt_embeddings(tokens)\n",
    "        x = self.tgt_positional_encoding(x)\n",
    "        x = self.decoder(x)\n",
    "        return x\n",
    "        \n",
    "    def project(self):\n",
    "        return self.projection_layer(x)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2183645-addc-43a7-ad78-98b5d1d60d6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_transformer():\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f646dafc-12aa-4a91-b124-3cdac124ca88",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
