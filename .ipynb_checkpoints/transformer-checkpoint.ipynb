{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "94292d3b-ca63-4b4c-823b-9dca4d4a4e4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9059a39-fabd-4219-937c-d25433a3df1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embeddings(nn.Module):\n",
    "    def __init__(self, d_model):\n",
    "        super().__init__\n",
    "        self.d_model = d_model\n",
    "        self.vocab_size = vocab_size \n",
    "        self.embedding = nn.Embedding(vocab_size, d_model) #initializes embeddings to be learned to training process\n",
    "    def forward(self,x):\n",
    "        return self.embedding(x) * math.sqrt(d_model) #normalize the variance of the embeddings\n",
    "\n",
    "\n",
    "#if \n",
    "\n",
    "#if feature=2i, sin(pos/1000^(2i/d_model), if feature=2i+k, cos(pos/1000^(2i/d_model)\n",
    "def Positional_Encoding(nn.Module):\n",
    "    def __init(self,seq_length,d_model,Pdrop):\n",
    "        self.dropout = nn.Droupout(Pdrop)\n",
    "        #if you add self before, then it gets learned as a parameter for the model\n",
    "        pos = torch.arange(0,seq_length).unsqueeze(1) #pos --> seq_length_pos x 1, adds an extra dimension\n",
    "        div_term = torch.exp(torch.arange(0,d_model,2)*np.log(1000)/d_model) #div term --> 1 x div_term_i\n",
    "        #so there is a different pe value for each token (pos) and feature (2i,2i+1)\n",
    "        pe = np.zeros(seq_length, d_model)\n",
    "        pe[:,0:2] = torch.sin(pe/div_term)\n",
    "        pe[:,1:2] = torch.cos(pe/div_term)\n",
    "        pe = pe.unsqueeze(0) # --> (1, seq, d_model)\n",
    "        self.register_buffer('positional_encoder',pe) #self.positional_encoder is not a learnable parameter\n",
    "\n",
    "    def forward(self,x):\n",
    "        x = x + self.positional_encoder(x)\n",
    "        return self.dropout(x)\n",
    "\n",
    "\n",
    "\n",
    "class LayerNormalization(nn.Module):\n",
    "    def __init__(self):\n",
    "        self.a = nn.Parameter(torch.ones(1)) #1 dimension\n",
    "        self.bias = nn.Parameter(torch.zeros(1)) #1 dimension\n",
    "\n",
    "    def forward(x):\n",
    "        mean = x.mean(dim=1, KeepDim=True) #keep all dimensions\n",
    "        std = x.std(dim=1, KeepDim=True)\n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
